---
title: "Stat 288 Time Use Analysis"
author: "Nils Dahlin, Gian Cercena"
date: "4/17/2023"
output: html_document
---

# Examining the Time Use of Americans and Predicting Personal Features 


## Data Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries needed
library(dplyr)
library(rpart)
library(tree)

# activity_dat contains the data/features regarding the
# activities respondents did throughout the week as well
# as related variables
load(file = "34453-0001-Data.rda")
activity_dat_orig <- da34453.0001

# summary_dat contains the data/features regarding the
# individual repsondents, contains features like:
# part/full time status, income, job type, family stats, etc.
load(file = "34453-0008-Data.rda")
summary_dat_orig <- da34453.0008

# surveyq_dat contains responses from individuals to survey questions
# includes questions like: Years of college credit completed?
# Have you looked for work in the past year? How did you get your
# highschool diploma?
load(file = "34453-0004-Data.rda")
surveyq_dat_orig <- da34453.0004

# linear_dat contains the features used in the linear regression
load(file = "34453-0011-Data.rda")
linear_dat_orig <- da34453.0011
```


### Pulling Features We Need/Looking at the Raw Data
```{r reducing dataset}
activity_dat <- activity_dat_orig %>%
    select(
        TUCASEID, TUACT_N, TEWHERE, TUACTDUR,
        TUT1CODE, TUT2CODE, TUT3CODE, TRCODE
    )

summary_dat <- summary_dat_orig %>%
    select(
        TUCASEID, TEAGE, TESEX, PEEDUCA, GTMETSTA,
        TELFS, TRDPFTPT, TRERNWA, T010101:T189999
    )

surveyq_dat <- surveyq_dat_orig %>%
    select(
        TUCASEID, HEHOUSUT, HETENURE, HRNUMHOU, 
        PEABSRSN, PECYC, PEJHRSN, PEDIPGED
    )

linear_dat <- linear_dat_orig %>%
    select(
        TUCASEID,
        LEGNHTH, LEPAIN, LRADJ, LUADDY, LUADHR, 
        LUADLOC, LULEAVE, LULVHRS, LULVYTD, LUPAID, 
        LUPDBRTH, LUPDCC, LUPDEC, LUPDERR, LUPDFMIL, 
        LUPDOIL, LUPDVAC, LUPTHOL, LUPTO, LUPTOHOL, 
        LUPTOMAT, LUPTOSCK, LUPTPSL, LUPTSCK, LUPTVAC
    )

head(activity_dat)
head(summary_dat)
head(surveyq_dat)
head(linear_dat)
```

### Renaming Certain Features/Columns
If this code chunk has errors, rerun one above
```{r renaming features}
# in activity_dat the three TUTCODE's are the codes for the
# primary, secondary, and tertiary activity codes which combined
# make the 6 digit activity code for each specific activity measured
# see here https://www.bls.gov/tus/lexicons/lexiconnoex0321.pdf)
activity_dat <- activity_dat %>%
    rename(
        "id" = "TUCASEID",
        "activity_num" = "TUACT_N",
        "location" = "TEWHERE",
        "duration" = "TUACTDUR",
        "code1" = "TUT1CODE",
        "code2" = "TUT2CODE",
        "code3" = "TUT3CODE",
        "activity_code" = "TRCODE"
    )
head(activity_dat)

# renaming non-activity code features
summary_dat <- summary_dat %>%
    rename(
        "id" = "TUCASEID",
        "age" = "TEAGE",
        "sex" = "TESEX",
        "education" = "PEEDUCA",
        "metro_area" = "GTMETSTA",
        "employment_status" = "TELFS",
        "employment_level" = "TRDPFTPT",
        "weekly_income" = "TRERNWA"
    )
head(summary_dat)

surveyq_dat <- surveyq_dat %>%
    rename(
        "id" = "TUCASEID",
        "type_of_housing" = "HEHOUSUT",
        "ownership_of_home" = "HETENURE",
        "num_in_household" = "HRNUMHOU",
        "missed_work" = "PEABSRSN",
        "yrs_of_collegecred" = "PECYC",
        "how_hs_diploma" = "PEDIPGED",
        "left_last_job" = "PEJHRSN"
    )
head(surveyq_dat)

linear_dat <- linear_dat %>%
    rename(
        "id" = "TUCASEID",
    )

# Add two decimal places to weekly_income
summary_dat$weekly_income <- summary_dat$weekly_income / 100
```

```{r testing dat}
dim(surveyq_dat)
surveyq_dat[duplicated(surveyq_dat$TUCASEID),]

surveyq_dat[]
```


### Combining Datasets
need to remove duplicates from surveyq_dat due to yrs_of_collegecred (keep non blank entries for duplicate id's)
```{r combine data}
summary_dat <- summary_dat[!duplicated(summary_dat), ]
surveyq_dat <- surveyq_dat[!duplicated(surveyq_dat), ]
dim(summary_dat)
dim(surveyq_dat)
```


## Linear Regression
### Set Up
```{r data set up}
# Want to find if flexibility in jobs is correlated with
# the income of the individual

# getting all salary data from summary_dat
salary_dat <- summary_dat %>%
    select(id, weekly_income)

# matching the salary ids and the linear_dat ids
linear_dat <- left_join(linear_dat, salary_dat, by = "id")
dim(linear_dat)

linear_dat <- linear_dat %>%
  select(-id)

# linear_dat$weekly_income <- linear_dat$weekly_income / 100

# removing rows with no income data
linear_dat <- linear_dat[!is.na(linear_dat$weekly_income), ]
dim(linear_dat)

head(linear_dat)
```

```{r linear train test}
# training/testing data
set.seed(47)
lin_train <- sample(1:nrow(linear_dat), 0.8 * nrow(linear_dat))
lin_test <- setdiff(1:nrow(linear_dat), lin_train)

# making training and testing data
lin_train_dat <- linear_dat[lin_train, ]
lin_test_dat <- linear_dat[lin_test, ]
```

### Statistics About the Dataset
```{r}
# average weekly income across entire dataset, as well as
# sd
mean(linear_dat$weekly_income)
median(linear_dat$weekly_income)
sd(linear_dat$weekly_income)
```

```{r}
# density plot of weekly income
plot(density(linear_dat$weekly_income))
```

### Making the Model
```{r}
# making the linear model
head(lin_train_dat)
lm1 <- lm(weekly_income ~ ., data = lin_train_dat)
summary(lm1)
```

```{r}
# testing the model'
pred <- predict(lm1, lin_test_dat)
mse <- mean((pred - lin_test_dat$weekly_income)^2)
mse
sqrt(mse)
```

### PCA
```{r pca}
# turn all data into numeric
lin_pca <- as.data.frame(lapply(linear_dat, as.numeric))
head(lin_pca)
# run pca over linear_dat to 2 dimensions
pca <- prcomp(lin_pca, scale = TRUE)
summary(pca)
```

```{r}
# graphing cumulative variance
plot(pca, type = "l")
```

```{r}
# plotting two dimensions
plot(pca$x[,1], pca$x[,2])
```

### Cluster Analysis on PCA
```{r}
# run cluster analysis on pca
set.seed(74)
kmeans_pca <- kmeans(pca$x, centers = 3)
kmeans_pca
```

```{r}
# red yellow green
colors <- c("red", "orange", "green") 
# plot the clusters with labels for which cluster they are
plot(pca$x[,1], pca$x[,2], col = colors[kmeans_pca$cluster])
# find the average pca values for each cluster and plot them a different yellow
# points(kmeans_pca$centers[,1], kmeans_pca$centers[,2], col = "yellow", pch = 8, cex = 2)
# print the centers in 2d
text(kmeans_pca$centers[,1], kmeans_pca$centers[,2], labels = 1:3, col = "black")
```


```{r}
# This and the following code block were originally meant to analyze some data
# but it wasn't meaningful because the data categories weren't ordinal
# The dataset avg_dat_graph is needed though, so these blocks still need to
# be run.

# get the average values in the linear data for each cluster
cluster1 <- linear_dat[kmeans_pca$cluster == 1, ]
cluster2 <- linear_dat[kmeans_pca$cluster == 2, ]
cluster3 <- linear_dat[kmeans_pca$cluster == 3, ]

# convert the cluster columnsn to numeric
cluster1 <- as.data.frame(lapply(cluster1, as.numeric))
cluster2 <- as.data.frame(lapply(cluster2, as.numeric))
cluster3 <- as.data.frame(lapply(cluster3, as.numeric))

# get the average values for each cluster
avg1 <- colMeans(cluster1)
avg2 <- colMeans(cluster2)
avg3 <- colMeans(cluster3)

# combine the averages into a dataframe
avg_dat <- data.frame(avg1, avg2, avg3)

# transpose the dataframe
avg_dat <- t(avg_dat)

# rename the columns
rownames(avg_dat) <- c("cluster1", "cluster2", "cluster3")

avg_dat

```

```{r}
library(ggplot2)
library(tidyr)
# bar plot each variable next to each other
# barplot(avg_dat, beside = TRUE, legend = TRUE, col = c("red", "blue", "green"))
# using ggplot
# side by side bar plot for each variable for each cluster with the y axis being the average value

# convert the avg_dat to a dataframe
avg_dat_graph <- as.data.frame(avg_dat)

# add a column for the cluster
avg_dat_graph$cluster <- rownames(avg_dat_graph)

# convert the dataframe to long format
avg_dat_graph <- gather(avg_dat_graph, variable, value, -cluster)

# # plot the data
# 
# ggplot(avg_dat_graph, aes(x = variable, y = value, fill = cluster)) +
#   geom_bar(stat = "identity", position = "dodge") +
#       labs(title = "Average Values of Variables for Each Cluster", x = "Variables", y = "Average Value") +
#     theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# get the average salary for each cluster
avg_sal <- linear_dat %>%
  group_by(kmeans_pca$cluster) %>%
  summarise(avg_sal = mean(weekly_income))

# plot the average salary for each cluster with the colors
ggplot(avg_dat_graph, aes(x = cluster, y = value, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
      labs(title = "Average Salary for Each Cluster", x = "Cluster", y = "Average Salary") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r}
# get average salary across entire dataset
avg_sal_all <- mean(linear_dat$weekly_income)
avg_sal_all
```

### Cluster Plotting
```{r}
cluster_1_rows <- linear_dat[kmeans_pca$cluster == 1, ]
cluster_2_rows <- linear_dat[kmeans_pca$cluster == 2, ]
cluster_3_rows <- linear_dat[kmeans_pca$cluster == 3, ]

linear_dat
```

```{r}
# creating a dataframe with the level of lepain and the count for each cluster
# get the unique values for lepain
lepain_vals <- unique(linear_dat$LEPAIN)

# get the counts for each value for each cluster
lepain_1 <- data.frame(table(cluster_1_rows$LEPAIN))
lepain_2 <- data.frame(table(cluster_2_rows$LEPAIN))
lepain_3 <- data.frame(table(cluster_3_rows$LEPAIN))

# add the cluster number to each dataframe
lepain_1 <- mutate(lepain_1, cluster = 1)
lepain_2 <- mutate(lepain_2, cluster = 2)
lepain_3 <- mutate(lepain_3, cluster = 3)


# rename the columns
lepain_1 <- rename(lepain_1, lepain = Var1, Freq = Freq)
lepain_2 <- rename(lepain_2, lepain = Var1, Freq = Freq)
lepain_3 <- rename(lepain_3, lepain = Var1, Freq = Freq)

# merge the dataframes
lepain_dat <- rbind(lepain_1, lepain_2, lepain_3)
lepain_dat

# change cluster to factor
lepain_dat$cluster <- as.factor(lepain_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(lepain_dat, aes(x = lepain, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = lepain_dat$lepain) +
  labs(title = "Count of Each Value for lepain for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
LRADJ_vals <- unique(linear_dat$LRADJ)

# get the counts for each value for each cluster
LRADJ_1 <- data.frame(table(cluster_1_rows$LRADJ))
LRADJ_2 <- data.frame(table(cluster_2_rows$LRADJ))
LRADJ_3 <- data.frame(table(cluster_3_rows$LRADJ))

# add the cluster number to each dataframe
LRADJ_1 <- mutate(LRADJ_1, cluster = 1)
LRADJ_2 <- mutate(LRADJ_2, cluster = 2)
LRADJ_3 <- mutate(LRADJ_3, cluster = 3)


# rename the columns
LRADJ_1 <- rename(LRADJ_1, LRADJ = Var1, Freq = Freq)
LRADJ_2 <- rename(LRADJ_2, LRADJ = Var1, Freq = Freq)
LRADJ_3 <- rename(LRADJ_3, LRADJ = Var1, Freq = Freq)

# merge the dataframes
LRADJ_dat <- rbind(LRADJ_1, LRADJ_2, LRADJ_3)
LRADJ_dat

# change cluster to factor
LRADJ_dat$cluster <- as.factor(LRADJ_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LRADJ_dat, aes(x = LRADJ, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LRADJ for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
LUADDY_vals <- unique(linear_dat$LUADDY)

# get the counts for each value for each cluster
LUADDY_1 <- data.frame(table(cluster_1_rows$LUADDY))
LUADDY_2 <- data.frame(table(cluster_2_rows$LUADDY))
LUADDY_3 <- data.frame(table(cluster_3_rows$LUADDY))

# add the cluster number to each dataframe
LUADDY_1 <- mutate(LUADDY_1, cluster = 1)
LUADDY_2 <- mutate(LUADDY_2, cluster = 2)
LUADDY_3 <- mutate(LUADDY_3, cluster = 3)


# rename the columns
LUADDY_1 <- rename(LUADDY_1, LUADDY = Var1, Freq = Freq)
LUADDY_2 <- rename(LUADDY_2, LUADDY = Var1, Freq = Freq)
LUADDY_3 <- rename(LUADDY_3, LUADDY = Var1, Freq = Freq)

# merge the dataframes
LUADDY_dat <- rbind(LUADDY_1, LUADDY_2, LUADDY_3)
LUADDY_dat

# change cluster to factor
LUADDY_dat$cluster <- as.factor(LUADDY_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUADDY_dat, aes(x = LUADDY, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUADDY for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Logisitic Regression - Part/Full-Time
### Set Up
```{r glm setup}
set.seed(0)

# combine data
temp <- surveyq_dat %>% select(id, type_of_housing, ownership_of_home, num_in_household)
temp <- temp[!duplicated(temp), ]
temp <- left_join(summary_dat, temp, by = "id")
dim(temp)
# grab only rows containing Full and Part Time Individuals
logreg_dat = temp[temp$employment_level=="(1) Full time"|temp$employment_level=="(2) Part time",]
logreg_dat = logreg_dat %>% select(-(T010101:T189999))
logreg_dat$employment_level = factor(logreg_dat$employment_level)

# training/testing data
summ_rows <- nrow(summary_dat)
train_samp <- sample(1:summ_rows, summ_rows * .8)
train <- logreg_dat[train_samp, ] %>% select(-id)
test <- logreg_dat[-train_samp, ] %>% select(-id)

dim(train)
dim(test)
unique(logreg_dat$employment_level)
```
### GLM Model
```{r logreg model}
logreg <- glm(formula=employment_level~., data = train, family = "binomial")

```

```{r}
summary(logreg)
```
```{r}
test_pred = predict(logreg, test)
actual = test$employment_level
```


## Random Forest
### Set up
```{r}
# Remove all NA's and -1 from weekly_income
rf_dat <- summary_dat %>%
    filter(weekly_income != -0.01) %>%
    filter(!is.na(weekly_income))
```

```{r}
# importing the randomForest library
library(randomForest)
```

### Random Forest Regression

```{r}
# Removing column 1 - 7 from rf_dat
rfr_dat <- rf_dat %>%
    select(-c(1:7))

# Creating the training and testing data sets
set.seed(456)
train_index <- sample(1:nrow(rfr_dat), size = 0.75 * nrow(rfr_dat))
rfr_train <- rfr_dat[train_index, ]
rfr_test <- rfr_dat[-train_index, ]
```

```{r}
# Creating the random forest classifier
rfr_model <- randomForest(weekly_income ~ ., data = rfr_train, ntree = 500)
rfr_model
```

```{r}
# Predicting the income bracket of the test data set
rfr_pred <- predict(rfr_model, rfr_test)
# rf_pred
```

```{r}
# Calculate MSE
rfr_mse <- mean((test$weekly_income - rfr_pred)^2)
rfr_mse
rfr_rmse <- sqrt(rfr_mse)
rfr_rmse
# Calculate R^2
rfr_r2 <- 1 - (rfr_mse / var(test$weekly_income))
rfr_r2
```

## RF Classifier

```{r random forest classifier creating column}
# Creating income brackets
# Dividing weekly_income into 5 brackets
brackets <- quantile(rf_dat$weekly_income, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))
brackets
```

```{r}
# Add income bracket to rf_dat
rfc_dat <- rf_dat %>%
    mutate(income_bracket = case_when(
        weekly_income <= brackets[2] ~ "very low",
        weekly_income > brackets[2] & weekly_income <= brackets[3] ~ "low",
        weekly_income > brackets[3] & weekly_income <= brackets[4] ~ "medium",
        weekly_income > brackets[4] & weekly_income <= brackets[5] ~ "high",
        weekly_income > brackets[5] ~ "very high"
    ))
# Moving income_bracket to the front of the data frame
rfc_dat <- rfc_dat %>%
    select(income_bracket, everything())
# convert income_bracket to factor
rfc_dat$income_bracket <- as.factor(rfc_dat$income_bracket)
# Order the factor levels for income
rfc_dat$income_bracket <- ordered(rfc_dat$income_bracket,
    levels =
        c("very low", "low", "medium", "high", "very high")
)
# removing columns 2-8
rfc_dat <- rfc_dat %>%
    select(-c(2:8))
```

```{r}
# Print the income bracket types and their factor as well as their counts
table(rfc_dat$income_bracket)
# print total number of observations
nrow(rfc_dat)
# find the total number of variables
ncol(rfc_dat) - 1
```

```{r}
# Remove weekly_income from rfc_dat
rfc_dat <- rfc_dat %>%
    select(-weekly_income)
```

```{r}
# Creating the training and testing data sets
# set.seed(456)
train_index <- sample(1:nrow(rfc_dat), size = 0.75 * nrow(rfc_dat))
rfc_train <- rfc_dat[train_index, ]
rfc_test <- rfc_dat[-train_index, ]
```

```{r}
# Creating the random forest classifier
rfc_model <- randomForest(income_bracket ~ ., data = rfc_train, ntree = 500)
# rfc_model
```

```{r}
# getting the values from the model
rfc_model$confusion
```

```{r}
plot(rfc_model)
# you can find out which line is which by matching the error rate at the end
# with the classification error
```

```{r}
# Predicting the income bracket of the test data set
rfc_pred <- predict(rfc_model, rfc_test)
# rfc_pred
```

```{r}
# import confusionMatrix
library(caret)
```

```{r}
# Confusion matrix
confusionMatrix(rfc_pred, rfc_test$income_bracket)
```

```{r}
rfc_model$confusion
```

```{r}
# find most important features and make plot bigger
varImpPlot(rfc_model, main = "Random Forest Classifier", n.var = 10)
```

```{r}
# Find the average values of each feature for each income bracket
rfc_dat_avg <- rfc_dat %>%
    group_by(income_bracket) %>%
    summarise_all(mean)
rfc_dat_avg
```

```{r}
# export to csv to perform row analysis in python
# since I can't figure it out in R
write.csv(rfc_dat_avg, "rfc_dat_avg.csv")
```

## Logistic Regression
```{r Log Reg setup}
# PRFTLF
# Reading in the 4th file
load(file = "34453-0004-Data.rda")
employ_dat_orig <- da34453.0004

employ_dat <- employ_dat_orig %>%
    select(TUCASEID, PRFTLF)



activity_dat <- activity_dat %>%
    select(
        TUCASEID, TUACT_N, TEWHERE, TUACTDUR,
        TUT1CODE, TUT2CODE, TUT3CODE, TRCODE
    )

summary_dat <- summary_dat %>%
    select(
        TUCASEID, TEAGE, TESEX, PEEDUCA, GTMETSTA,
        TELFS, TRDPFTPT, TRERNWA, T010101:T189999
    )

head(activity_dat)
head(summary_dat)
```