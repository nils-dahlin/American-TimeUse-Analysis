---
title: "Stat 288 Time Use Analysis"
author: "Nils Dahlin, Gian Cercena"
date: "4/17/2023"
output: html_document
---

# Examining the Time Use of Americans and Predicting Personal Features 


## Data Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries needed
library(dplyr)
library(rpart)
library(tree)
library(ggplot2)
library(tidyr)
library(randomForest)
library(glmnet)
library(ranger)
library(corrplot)
library(car)
library(pROC)
library(dbscan)

# activity_dat contains the data/features regarding the
# activities respondents did throughout the week as well
# as related variables
load(file = "34453-0001-Data.rda")
activity_dat_orig <- da34453.0001

# summary_dat contains the data/features regarding the
# individual repsondents, contains features like:
# part/full time status, income, job type, family stats, etc.
load(file = "34453-0008-Data.rda")
summary_dat_orig <- da34453.0008

# surveyq_dat contains responses from individuals to survey questions
# includes questions like: Years of college credit completed?
# Have you looked for work in the past year? How did you get your
# highschool diploma?
load(file = "34453-0004-Data.rda")
surveyq_dat_orig <- da34453.0004

# linear_dat contains the features used in the linear regression
load(file = "34453-0011-Data.rda")
linear_dat_orig <- da34453.0011
```


### Pulling Features We Need/Looking at the Raw Data
```{r reducing dataset}
activity_dat <- activity_dat_orig %>%
    select(
        TUCASEID, TUACT_N, TEWHERE, TUACTDUR,
        TUT1CODE, TUT2CODE, TUT3CODE, TRCODE
    )

summary_dat <- summary_dat_orig %>%
    select(
        TUCASEID, TEAGE, TESEX, PEEDUCA, GTMETSTA,
        TELFS, TRDPFTPT, TRERNWA, T010101:T189999
    )

surveyq_dat <- surveyq_dat_orig %>%
    select(
        TUCASEID, HEHOUSUT, HETENURE, HRNUMHOU,
        PEABSRSN, PECYC, PEJHRSN, PEDIPGED, PEAFEVER,
        PEDISREM, PEDISDRS, PEDISEAR, PEDISEYE, PEDISOUT,
        PEDISPHY, PEERNLAB
    )

linear_dat <- linear_dat_orig %>%
    select(
        TUCASEID,
        # LEGNHTH, LEPAIN,
        LRADJ, LUADDY, LUADHR,
        LUADLOC, LULEAVE, LULVHRS, LULVYTD, LUPAID,
        LUPDBRTH, LUPDCC, LUPDEC, LUPDERR, LUPDFMIL,
        LUPDOIL, LUPDVAC, LUPTHOL, LUPTO, LUPTOHOL,
        LUPTOMAT, LUPTOSCK, LUPTPSL, LUPTSCK, LUPTVAC,
        LUUNBRTH, LUUNCC, LUUNEC, LUUNERR, LUUNEVR,
        LUUNEVR1:LUUNEVR7, LUUNFMIL, LUUNOIL, LUUNPD,
    )

head(activity_dat)
head(summary_dat)
head(surveyq_dat)
head(linear_dat)
```

### Renaming Certain Features/Columns
If this code chunk has errors, rerun one above
```{r renaming features}
# in activity_dat the three TUTCODE's are the codes for the
# primary, secondary, and tertiary activity codes which combined
# make the 6 digit activity code for each specific activity measured
# see here https://www.bls.gov/tus/lexicons/lexiconnoex0321.pdf)
activity_dat <- activity_dat %>%
    rename(
        "id" = "TUCASEID",
        "activity_num" = "TUACT_N",
        "location" = "TEWHERE",
        "duration" = "TUACTDUR",
        "code1" = "TUT1CODE",
        "code2" = "TUT2CODE",
        "code3" = "TUT3CODE",
        "activity_code" = "TRCODE"
    )
head(activity_dat)

# renaming non-activity code features
summary_dat <- summary_dat %>%
    rename(
        "id" = "TUCASEID",
        "age" = "TEAGE",
        "sex" = "TESEX",
        "education" = "PEEDUCA",
        "metro_area" = "GTMETSTA",
        "employment_status" = "TELFS",
        "employment_level" = "TRDPFTPT",
        "weekly_income" = "TRERNWA"
    )
head(summary_dat)

surveyq_dat <- surveyq_dat %>%
    rename(
        "id" = "TUCASEID",
        "type_of_housing" = "HEHOUSUT",
        "ownership_of_home" = "HETENURE",
        "num_in_household" = "HRNUMHOU",
        "missed_work" = "PEABSRSN",
        "yrs_of_collegecred" = "PECYC",
        "how_hs_diploma" = "PEDIPGED",
        "left_last_job" = "PEJHRSN",
        "active_duty" = "PEAFEVER",
        "memory_issues" = "PEDISREM",
        "dressing_issues" = "PEDISDRS",
        "hearing_issues" = "PEDISEAR",
        "eye_issues" = "PEDISEYE",
        "issues_errands" = "PEDISOUT",
        "issues_walking" = "PEDISPHY",
        "unionized" = "PEERNLAB"
    )
head(surveyq_dat)

linear_dat <- linear_dat %>%
    rename(
        "id" = "TUCASEID",
    )

# Add two decimal places to weekly_income
summary_dat$weekly_income <- summary_dat$weekly_income / 100
```
### Surveyq Test
```{r testing dat}
dim(surveyq_dat)
surveyq_dat[duplicated(surveyq_dat$TUCASEID), ]

surveyq_dat[]
```


### Combining Datasets
need to remove duplicates from surveyq_dat due to yrs_of_collegecred (keep non blank entries for duplicate id's)
```{r combine data}
summary_dat <- summary_dat[!duplicated(summary_dat), ]
surveyq_dat <- surveyq_dat[!duplicated(surveyq_dat), ]
dim(summary_dat)
dim(surveyq_dat)
```


## Linear Regression
### Set Up
```{r data set up}
# Want to find if flexibility in jobs is correlated with
# the income of the individual

# getting all salary data from summary_dat
salary_dat <- summary_dat %>%
    select(id, weekly_income)

# matching the salary ids and the linear_dat ids
linear_dat <- left_join(linear_dat, salary_dat, by = "id")
dim(linear_dat)

linear_dat <- linear_dat %>%
    select(-id)

# linear_dat$weekly_income <- linear_dat$weekly_income / 100

# removing rows with no income data
linear_dat <- linear_dat[!is.na(linear_dat$weekly_income), ]
dim(linear_dat)

head(linear_dat)
```

```{r}
# making the "(-1) Blank" entries NA
# linear_dat[linear_dat == "(-1) Blank"] <- NA
# linear_dat
# Finding all columns that doesn't have >=2 unique values
# and removing them
# linear_dat <- linear_dat[, sapply(linear_dat, function(x) length(unique(x))) >= 2]
# linear_dat
# # printing the unique values for each column
# for (i in 1:ncol(linear_dat)) {
#     print(unique(linear_dat[, i]))
# }
```

```{r linear train test}
# training/testing data
# set.seed(47)
lin_train <- sample(1:nrow(linear_dat), 0.8 * nrow(linear_dat))
lin_test <- setdiff(1:nrow(linear_dat), lin_train)

# making training and testing data
lin_train_dat <- linear_dat[lin_train, ]
lin_test_dat <- linear_dat[lin_test, ]
```

### Statistics About the Dataset
```{r}
# average weekly income across entire dataset, as well as
# sd
mean(linear_dat$weekly_income)
median(linear_dat$weekly_income)
sd(linear_dat$weekly_income)
```

```{r}
# density plot of weekly income
plot(density(linear_dat$weekly_income))
```

### Making the Model
```{r}
# making the linear model
head(lin_train_dat)
lm1 <- lm(weekly_income ~ ., data = lin_train_dat)
summary(lm1)
```

```{r echo = FALSE results = hide}
# Using stepwise variable selection to make a more interpretable model
# This steps both forwads and back
step_lm1 <- step(lm1, direction = "both")
plot(step_lm1)
```

```{r}
plot(step_lm1$anova$AIC, xlab = "Number of variables", ylab = "AIC")
lin_train_dat[575, ]
```

```{r}
step_lm1
# This is the best model according to AIC
lm2 <- lm(weekly_income ~ LUADDY + LUADHR + LUADLOC + LUPAID +
    LUPDBRTH + LUPDFMIL + LUPDOIL + LUPTO + LUPTOHOL + LUPTOSCK +
    LUPTSCK + LUPTVAC, data = lin_train_dat)
```

```{r}
summary(lm2)
```

```{r}
# testing the model
pred <- predict(lm2, lin_test_dat)
mse <- mean((pred - lin_test_dat$weekly_income)^2)
mse
sqrt(mse)
```

### PCA
```{r pca}
# turn all data into numeric
lin_pca <- as.data.frame(lapply(linear_dat, as.numeric))
head(lin_pca)
```

```{r}
# standardize the data
set.seed(1)
lin_pca <- scale(lin_pca)
```

```{r}
# run pca over linear_dat to 2 dimensions
set.seed(1)
pca <- prcomp(lin_pca, scale = TRUE)
summary(pca)
```

```{r}
variance_prop <- pca$sdev^2 / sum(pca$sdev^2)
cumulative_prop <- cumsum(variance_prop)
plot_data <- data.frame(
    PC = 1:length(variance_prop),
    Cumulative_prop = cumulative_prop
)

ggplot(plot_data, aes(x = PC, y = Cumulative_prop)) +
    geom_line() +
    geom_point() +
    scale_x_continuous("Principal Component") +
    scale_y_continuous("Cumulative Proportion of Variance Explained",
        limits = c(0, 1)
    ) +
    theme_minimal() +
    geom_vline(xintercept = 7, linetype = "dashed")
```

```{r}
# plotting two dimensions
plot(pca$x[, 1], pca$x[, 2])
```

```{r}
# finding the most important features

# getting the loadings
loadings <- pca$rotation
# getting the absolute value of the loadings
loadings <- abs(loadings)
# getting the sum of the absolute value of the loadings
loadings <- apply(loadings, 1, sum)
# getting the names of the features
loadings <- data.frame(
    feature = names(loadings),
    loadings = loadings
)

# sorting the loadings
loadings <- loadings[order(-loadings$loadings), ]
loadings
head(loadings)
```

### Cluster Analysis on PCA
```{r}
# run cluster analysis on pca
best_seed <- 0
best_kmeans_pca <- NULL
# running a for loop over different set seeds to find the best

for (i in 1:100) {
    set.seed(i)
    kmeans_pca <- kmeans(pca$x[, 1:2], centers = 3)
    if (i == 1) {
        best_kmeans_pca <- kmeans_pca
        best_seed <- i
    } else if (kmeans_pca$tot.withinss < best_kmeans_pca$tot.withinss) {
        best_kmeans_pca <- kmeans_pca
        best_seed <- i
    }
}
best_seed
```

```{r}
set.seed(best_seed)
kmeans_pca <- kmeans(pca$x[, 1:2], centers = 3)
kmeans_pca
```

```{r}
# red yellow green
colors <- c("red", "orange", "green")
# plot the clusters with labels for which cluster they are
plot(pca$x[, 1], pca$x[, 2], col = colors[kmeans_pca$cluster])
# putting the labels on the center of the clusters
text(kmeans_pca$centers[, 1], kmeans_pca$centers[, 2], labels = 1:3, col = "black")
```


```{r}
# This and the following code block were originally meant to analyze some data
# but it wasn't meaningful because the data categories weren't ordinal
# The dataset avg_dat_graph is needed though, so these blocks still need to
# be run.

# get the average values in the linear data for each cluster
cluster1 <- linear_dat[kmeans_pca$cluster == 1, ]
cluster2 <- linear_dat[kmeans_pca$cluster == 2, ]
cluster3 <- linear_dat[kmeans_pca$cluster == 3, ]

# convert the cluster columnsn to numeric
cluster1 <- as.data.frame(lapply(cluster1, as.numeric))
cluster2 <- as.data.frame(lapply(cluster2, as.numeric))
cluster3 <- as.data.frame(lapply(cluster3, as.numeric))

# get the average values for each cluster
avg1 <- colMeans(cluster1)
avg2 <- colMeans(cluster2)
avg3 <- colMeans(cluster3)

# combine the averages into a dataframe
avg_dat <- data.frame(avg1, avg2, avg3)

# transpose the dataframe
avg_dat <- t(avg_dat)

# rename the columns
rownames(avg_dat) <- c("cluster1", "cluster2", "cluster3")

avg_dat
```

```{r}
# convert the avg_dat to a dataframe
avg_dat_graph <- as.data.frame(avg_dat)

# add a column for the cluster
avg_dat_graph$cluster <- rownames(avg_dat_graph)

# convert the dataframe to long format
avg_dat_graph <- gather(avg_dat_graph, variable, value, -cluster)

# ggplot(avg_dat_graph, aes(x = variable, y = value, fill = cluster)) +
#   geom_bar(stat = "identity", position = "dodge") +
#       labs(title = "Average Values of Variables for Each Cluster", x = "Variables", y = "Average Value") +
#     theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# get the average salary for each cluster
avg_sal <- linear_dat %>%
    group_by(kmeans_pca$cluster) %>%
    summarise(avg_sal = mean(weekly_income))

# plot the average salary for each cluster with the colors
ggplot(avg_dat_graph, aes(x = cluster, y = value, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Average Salary for Each Cluster", x = "Cluster", y = "Average Salary") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# get average salary across entire dataset
avg_sal_all <- mean(linear_dat$weekly_income)
avg_sal_all
```

### Cluster Plotting
```{r}
cluster_1_rows <- linear_dat[kmeans_pca$cluster == 1, ]
cluster_2_rows <- linear_dat[kmeans_pca$cluster == 2, ]
cluster_3_rows <- linear_dat[kmeans_pca$cluster == 3, ]

linear_dat
```

#### Top 5 Most Important Features from the PCA
##### LUUNEVR
```{r}
LUUNEVR_vals <- unique(linear_dat$LUUNEVR)

# get the counts for each value for each cluster
LUUNEVR_1 <- data.frame(table(cluster_1_rows$LUUNEVR))
LUUNEVR_2 <- data.frame(table(cluster_2_rows$LUUNEVR))
LUUNEVR_3 <- data.frame(table(cluster_3_rows$LUUNEVR))

# add the cluster number to each dataframe
LUUNEVR_1 <- mutate(LUUNEVR_1, cluster = 1)
LUUNEVR_2 <- mutate(LUUNEVR_2, cluster = 2)
LUUNEVR_3 <- mutate(LUUNEVR_3, cluster = 3)

# rename the columns
LUUNEVR_1 <- rename(LUUNEVR_1, LUUNEVR = Var1, Freq = Freq)
LUUNEVR_2 <- rename(LUUNEVR_2, LUUNEVR = Var1, Freq = Freq)
LUUNEVR_3 <- rename(LUUNEVR_3, LUUNEVR = Var1, Freq = Freq)

# merge the dataframes
LUUNEVR_dat <- rbind(LUUNEVR_1, LUUNEVR_2, LUUNEVR_3)
LUUNEVR_dat

# change cluster to factor
LUUNEVR_dat$cluster <- as.factor(LUUNEVR_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUUNEVR_dat, aes(x = LUUNEVR, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUUNEVR for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# plotting the percentage of each value for LUUNEVR for each cluster
# get the total number of rows for each cluster
LUUNEVR_total_1 <- nrow(cluster_1_rows)
LUUNEVR_total_2 <- nrow(cluster_2_rows)
LUUNEVR_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUUNEVR_1 <- mutate(LUUNEVR_1, percent = Freq / LUUNEVR_total_1)
LUUNEVR_2 <- mutate(LUUNEVR_2, percent = Freq / LUUNEVR_total_2)
LUUNEVR_3 <- mutate(LUUNEVR_3, percent = Freq / LUUNEVR_total_3)

# printing the values to make sure they equal 1
sum(LUUNEVR_1$percent)
sum(LUUNEVR_2$percent)
sum(LUUNEVR_3$percent)

# merge the dataframes
LUUNEVR_dat <- rbind(LUUNEVR_1, LUUNEVR_2, LUUNEVR_3)

# change cluster to factor
LUUNEVR_dat$cluster <- as.factor(LUUNEVR_dat$cluster)

# plot the data
ggplot(LUUNEVR_dat, aes(x = LUUNEVR, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUUNEVR_dat$LUUNEVR) +
  labs(title = "Percentage of Each Value for LUUNEVR for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
##### LUPTOHOL
```{r}
LUPTOHOL_vals <- unique(linear_dat$LUPTOHOL)

# get the counts for each value for each cluster
LUPTOHOL_1 <- data.frame(table(cluster_1_rows$LUPTOHOL))
LUPTOHOL_2 <- data.frame(table(cluster_2_rows$LUPTOHOL))
LUPTOHOL_3 <- data.frame(table(cluster_3_rows$LUPTOHOL))

# add the cluster number to each dataframe
LUPTOHOL_1 <- mutate(LUPTOHOL_1, cluster = 1)
LUPTOHOL_2 <- mutate(LUPTOHOL_2, cluster = 2)
LUPTOHOL_3 <- mutate(LUPTOHOL_3, cluster = 3)

# rename the columns
LUPTOHOL_1 <- rename(LUPTOHOL_1, LUPTOHOL = Var1, Freq = Freq)
LUPTOHOL_2 <- rename(LUPTOHOL_2, LUPTOHOL = Var1, Freq = Freq)
LUPTOHOL_3 <- rename(LUPTOHOL_3, LUPTOHOL = Var1, Freq = Freq)

# merge the dataframes
LUPTOHOL_dat <- rbind(LUPTOHOL_1, LUPTOHOL_2, LUPTOHOL_3)
LUPTOHOL_dat

# change cluster to factor
LUPTOHOL_dat$cluster <- as.factor(LUPTOHOL_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUPTOHOL_dat, aes(x = LUPTOHOL, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUPTOHOL for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# plotting the percentage of each value for LUPTOHOL for each cluster
# get the total number of rows for each cluster
LUPTOHOL_total_1 <- nrow(cluster_1_rows)
LUPTOHOL_total_2 <- nrow(cluster_2_rows)
LUPTOHOL_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUPTOHOL_1 <- mutate(LUPTOHOL_1, percent = Freq / LUPTOHOL_total_1)
LUPTOHOL_2 <- mutate(LUPTOHOL_2, percent = Freq / LUPTOHOL_total_2)
LUPTOHOL_3 <- mutate(LUPTOHOL_3, percent = Freq / LUPTOHOL_total_3)

# printing the values to make sure they equal 1
sum(LUPTOHOL_1$percent)
sum(LUPTOHOL_2$percent)
sum(LUPTOHOL_3$percent)

# merge the dataframes
LUPTOHOL_dat <- rbind(LUPTOHOL_1, LUPTOHOL_2, LUPTOHOL_3)

# change cluster to factor
LUPTOHOL_dat$cluster <- as.factor(LUPTOHOL_dat$cluster)

# plot the data
ggplot(LUPTOHOL_dat, aes(x = LUPTOHOL, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUPTOHOL_dat$LUPTOHOL) +
  labs(title = "Percentage of Each Value for LUPTOHOL for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
##### LUPAID
```{r}
LUPAID_vals <- unique(linear_dat$LUPAID)

# get the counts for each value for each cluster
LUPAID_1 <- data.frame(table(cluster_1_rows$LUPAID))
LUPAID_2 <- data.frame(table(cluster_2_rows$LUPAID))
LUPAID_3 <- data.frame(table(cluster_3_rows$LUPAID))

# add the cluster number to each dataframe
LUPAID_1 <- mutate(LUPAID_1, cluster = 1)
LUPAID_2 <- mutate(LUPAID_2, cluster = 2)
LUPAID_3 <- mutate(LUPAID_3, cluster = 3)

# rename the columns
LUPAID_1 <- rename(LUPAID_1, LUPAID = Var1, Freq = Freq)
LUPAID_2 <- rename(LUPAID_2, LUPAID = Var1, Freq = Freq)
LUPAID_3 <- rename(LUPAID_3, LUPAID = Var1, Freq = Freq)

# merge the dataframes
LUPAID_dat <- rbind(LUPAID_1, LUPAID_2, LUPAID_3)
LUPAID_dat

# change cluster to factor
LUPAID_dat$cluster <- as.factor(LUPAID_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUPAID_dat, aes(x = LUPAID, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUPAID for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# plotting the percentage of each value for LUPAID for each cluster
# get the total number of rows for each cluster
LUPAID_total_1 <- nrow(cluster_1_rows)
LUPAID_total_2 <- nrow(cluster_2_rows)
LUPAID_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUPAID_1 <- mutate(LUPAID_1, percent = Freq / LUPAID_total_1)
LUPAID_2 <- mutate(LUPAID_2, percent = Freq / LUPAID_total_2)
LUPAID_3 <- mutate(LUPAID_3, percent = Freq / LUPAID_total_3)

# printing the values to make sure they equal 1
sum(LUPAID_1$percent)
sum(LUPAID_2$percent)
sum(LUPAID_3$percent)

# merge the dataframes
LUPAID_dat <- rbind(LUPAID_1, LUPAID_2, LUPAID_3)

# change cluster to factor
LUPAID_dat$cluster <- as.factor(LUPAID_dat$cluster)

# plot the data
ggplot(LUPAID_dat, aes(x = LUPAID, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUPAID_dat$LUPAID) +
  labs(title = "Percentage of Each Value for LUPAID for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
##### LUPTSCK
```{r}
LUPTSCK_vals <- unique(linear_dat$LUPTSCK)

# get the counts for each value for each cluster
LUPTSCK_1 <- data.frame(table(cluster_1_rows$LUPTSCK))
LUPTSCK_2 <- data.frame(table(cluster_2_rows$LUPTSCK))
LUPTSCK_3 <- data.frame(table(cluster_3_rows$LUPTSCK))

# add the cluster number to each dataframe
LUPTSCK_1 <- mutate(LUPTSCK_1, cluster = 1)
LUPTSCK_2 <- mutate(LUPTSCK_2, cluster = 2)
LUPTSCK_3 <- mutate(LUPTSCK_3, cluster = 3)

# rename the columns
LUPTSCK_1 <- rename(LUPTSCK_1, LUPTSCK = Var1, Freq = Freq)
LUPTSCK_2 <- rename(LUPTSCK_2, LUPTSCK = Var1, Freq = Freq)
LUPTSCK_3 <- rename(LUPTSCK_3, LUPTSCK = Var1, Freq = Freq)

# merge the dataframes
LUPTSCK_dat <- rbind(LUPTSCK_1, LUPTSCK_2, LUPTSCK_3)
LUPTSCK_dat

# change cluster to factor
LUPTSCK_dat$cluster <- as.factor(LUPTSCK_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUPTSCK_dat, aes(x = LUPTSCK, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUPTSCK for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# plotting the percentage of each value for LUPTSCK for each cluster
# get the total number of rows for each cluster
LUPTSCK_total_1 <- nrow(cluster_1_rows)
LUPTSCK_total_2 <- nrow(cluster_2_rows)
LUPTSCK_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUPTSCK_1 <- mutate(LUPTSCK_1, percent = Freq / LUPTSCK_total_1)
LUPTSCK_2 <- mutate(LUPTSCK_2, percent = Freq / LUPTSCK_total_2)
LUPTSCK_3 <- mutate(LUPTSCK_3, percent = Freq / LUPTSCK_total_3)

# printing the values to make sure they equal 1
sum(LUPTSCK_1$percent)
sum(LUPTSCK_2$percent)
sum(LUPTSCK_3$percent)

# merge the dataframes
LUPTSCK_dat <- rbind(LUPTSCK_1, LUPTSCK_2, LUPTSCK_3)

# change cluster to factor
LUPTSCK_dat$cluster <- as.factor(LUPTSCK_dat$cluster)

# plot the data
ggplot(LUPTSCK_dat, aes(x = LUPTSCK, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUPTSCK_dat$LUPTSCK) +
  labs(title = "Percentage of Each Value for LUPTSCK for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
##### LUUNFMIL
```{r}
LUUNFMIL_vals <- unique(linear_dat$LUUNFMIL)

# get the counts for each value for each cluster
LUUNFMIL_1 <- data.frame(table(cluster_1_rows$LUUNFMIL))
LUUNFMIL_2 <- data.frame(table(cluster_2_rows$LUUNFMIL))
LUUNFMIL_3 <- data.frame(table(cluster_3_rows$LUUNFMIL))

# add the cluster number to each dataframe
LUUNFMIL_1 <- mutate(LUUNFMIL_1, cluster = 1)
LUUNFMIL_2 <- mutate(LUUNFMIL_2, cluster = 2)
LUUNFMIL_3 <- mutate(LUUNFMIL_3, cluster = 3)

# rename the columns
LUUNFMIL_1 <- rename(LUUNFMIL_1, LUUNFMIL = Var1, Freq = Freq)
LUUNFMIL_2 <- rename(LUUNFMIL_2, LUUNFMIL = Var1, Freq = Freq)
LUUNFMIL_3 <- rename(LUUNFMIL_3, LUUNFMIL = Var1, Freq = Freq)

# merge the dataframes
LUUNFMIL_dat <- rbind(LUUNFMIL_1, LUUNFMIL_2, LUUNFMIL_3)
LUUNFMIL_dat

# change cluster to factor
LUUNFMIL_dat$cluster <- as.factor(LUUNFMIL_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUUNFMIL_dat, aes(x = LUUNFMIL, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUUNFMIL for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# plotting the percentage of each value for LUUNFMIL for each cluster
# get the total number of rows for each cluster
LUUNFMIL_total_1 <- nrow(cluster_1_rows)
LUUNFMIL_total_2 <- nrow(cluster_2_rows)
LUUNFMIL_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUUNFMIL_1 <- mutate(LUUNFMIL_1, percent = Freq / LUUNFMIL_total_1)
LUUNFMIL_2 <- mutate(LUUNFMIL_2, percent = Freq / LUUNFMIL_total_2)
LUUNFMIL_3 <- mutate(LUUNFMIL_3, percent = Freq / LUUNFMIL_total_3)

# printing the values to make sure they equal 1
sum(LUUNFMIL_1$percent)
sum(LUUNFMIL_2$percent)
sum(LUUNFMIL_3$percent)

# merge the dataframes
LUUNFMIL_dat <- rbind(LUUNFMIL_1, LUUNFMIL_2, LUUNFMIL_3)

# change cluster to factor
LUUNFMIL_dat$cluster <- as.factor(LUUNFMIL_dat$cluster)

# plot the data
ggplot(LUUNFMIL_dat, aes(x = LUUNFMIL, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUUNFMIL_dat$LUUNFMIL) +
  labs(title = "Percentage of Each Value for LUUNFMIL for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
##### LUPDEC
```{r}
LUPDEC_vals <- unique(linear_dat$LUPDEC)

# get the counts for each value for each cluster
LUPDEC_1 <- data.frame(table(cluster_1_rows$LUPDEC))
LUPDEC_2 <- data.frame(table(cluster_2_rows$LUPDEC))
LUPDEC_3 <- data.frame(table(cluster_3_rows$LUPDEC))

# add the cluster number to each dataframe
LUPDEC_1 <- mutate(LUPDEC_1, cluster = 1)
LUPDEC_2 <- mutate(LUPDEC_2, cluster = 2)
LUPDEC_3 <- mutate(LUPDEC_3, cluster = 3)

# rename the columns
LUPDEC_1 <- rename(LUPDEC_1, LUPDEC = Var1, Freq = Freq)
LUPDEC_2 <- rename(LUPDEC_2, LUPDEC = Var1, Freq = Freq)
LUPDEC_3 <- rename(LUPDEC_3, LUPDEC = Var1, Freq = Freq)

# merge the dataframes
LUPDEC_dat <- rbind(LUPDEC_1, LUPDEC_2, LUPDEC_3)
LUPDEC_dat

# change cluster to factor
LUPDEC_dat$cluster <- as.factor(LUPDEC_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUPDEC_dat, aes(x = LUPDEC, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUPDEC for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# plotting the percentage of each value for LUPDEC for each cluster
# get the total number of rows for each cluster
LUPDEC_total_1 <- nrow(cluster_1_rows)
LUPDEC_total_2 <- nrow(cluster_2_rows)
LUPDEC_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUPDEC_1 <- mutate(LUPDEC_1, percent = Freq / LUPDEC_total_1)
LUPDEC_2 <- mutate(LUPDEC_2, percent = Freq / LUPDEC_total_2)
LUPDEC_3 <- mutate(LUPDEC_3, percent = Freq / LUPDEC_total_3)

# printing the values to make sure they equal 1
sum(LUPDEC_1$percent)
sum(LUPDEC_2$percent)
sum(LUPDEC_3$percent)

# merge the dataframes
LUPDEC_dat <- rbind(LUPDEC_1, LUPDEC_2, LUPDEC_3)

# change cluster to factor
LUPDEC_dat$cluster <- as.factor(LUPDEC_dat$cluster)

# plot the data
ggplot(LUPDEC_dat, aes(x = LUPDEC, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUPDEC_dat$LUPDEC) +
  labs(title = "Percentage of Each Value for LUPDEC for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### Additional Plots
##### LRADJ
```{r}
LRADJ_vals <- unique(linear_dat$LRADJ)

# get the counts for each value for each cluster
LRADJ_1 <- data.frame(table(cluster_1_rows$LRADJ))
LRADJ_2 <- data.frame(table(cluster_2_rows$LRADJ))
LRADJ_3 <- data.frame(table(cluster_3_rows$LRADJ))

# add the cluster number to each dataframe
LRADJ_1 <- mutate(LRADJ_1, cluster = 1)
LRADJ_2 <- mutate(LRADJ_2, cluster = 2)
LRADJ_3 <- mutate(LRADJ_3, cluster = 3)

# rename the columns
LRADJ_1 <- rename(LRADJ_1, LRADJ = Var1, Freq = Freq)
LRADJ_2 <- rename(LRADJ_2, LRADJ = Var1, Freq = Freq)
LRADJ_3 <- rename(LRADJ_3, LRADJ = Var1, Freq = Freq)

# merge the dataframes
LRADJ_dat <- rbind(LRADJ_1, LRADJ_2, LRADJ_3)
LRADJ_dat

# change cluster to factor
LRADJ_dat$cluster <- as.factor(LRADJ_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LRADJ_dat, aes(x = LRADJ, y = Freq, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Count of Each Value for LRADJ for Each Cluster", x = "Cluster", y = "Count") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for LRADJ for each cluster
# get the total number of rows for each cluster
LRADJ_total_1 <- nrow(cluster_1_rows)
LRADJ_total_2 <- nrow(cluster_2_rows)
LRADJ_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LRADJ_1 <- mutate(LRADJ_1, percent = Freq / LRADJ_total_1)
LRADJ_2 <- mutate(LRADJ_2, percent = Freq / LRADJ_total_2)
LRADJ_3 <- mutate(LRADJ_3, percent = Freq / LRADJ_total_3)

# printing the values to make sure they equal 1
sum(LRADJ_1$percent)
sum(LRADJ_2$percent)
sum(LRADJ_3$percent)

# merge the dataframes
LRADJ_dat <- rbind(LRADJ_1, LRADJ_2, LRADJ_3)

# change cluster to factor
LRADJ_dat$cluster <- as.factor(LRADJ_dat$cluster)

# plot the data
ggplot(LRADJ_dat, aes(x = LRADJ, y = percent, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge", group = LRADJ_dat$LRADJ) +
    labs(title = "Percentage of Each Value for LRADJ for Each Cluster", x = "Cluster", y = "Percentage") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

##### LUADDY
```{r}
LUADDY_vals <- unique(linear_dat$LUADDY)

# get the counts for each value for each cluster
LUADDY_1 <- data.frame(table(cluster_1_rows$LUADDY))
LUADDY_2 <- data.frame(table(cluster_2_rows$LUADDY))
LUADDY_3 <- data.frame(table(cluster_3_rows$LUADDY))

# add the cluster number to each dataframe
LUADDY_1 <- mutate(LUADDY_1, cluster = 1)
LUADDY_2 <- mutate(LUADDY_2, cluster = 2)
LUADDY_3 <- mutate(LUADDY_3, cluster = 3)

# rename the columns
LUADDY_1 <- rename(LUADDY_1, LUADDY = Var1, Freq = Freq)
LUADDY_2 <- rename(LUADDY_2, LUADDY = Var1, Freq = Freq)
LUADDY_3 <- rename(LUADDY_3, LUADDY = Var1, Freq = Freq)

# merge the dataframes
LUADDY_dat <- rbind(LUADDY_1, LUADDY_2, LUADDY_3)
LUADDY_dat

# change cluster to factor
LUADDY_dat$cluster <- as.factor(LUADDY_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUADDY_dat, aes(x = LUADDY, y = Freq, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Count of Each Value for LUADDY for Each Cluster", x = "Cluster", y = "Count") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for LUADDY for each cluster
# get the total number of rows for each cluster
LUADDY_total_1 <- nrow(cluster_1_rows)
LUADDY_total_2 <- nrow(cluster_2_rows)
LUADDY_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUADDY_1 <- mutate(LUADDY_1, percent = Freq / LUADDY_total_1)
LUADDY_2 <- mutate(LUADDY_2, percent = Freq / LUADDY_total_2)
LUADDY_3 <- mutate(LUADDY_3, percent = Freq / LUADDY_total_3)

# printing the values to make sure they equal 1
sum(LUADDY_1$percent)
sum(LUADDY_2$percent)
sum(LUADDY_3$percent)

# merge the dataframes
LUADDY_dat <- rbind(LUADDY_1, LUADDY_2, LUADDY_3)

# change cluster to factor
LUADDY_dat$cluster <- as.factor(LUADDY_dat$cluster)

# plot the data
ggplot(LUADDY_dat, aes(x = LUADDY, y = percent, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge", group = LUADDY_dat$LUADDY) +
    labs(title = "Percentage of Each Value for LUADDY for Each Cluster", x = "Cluster", y = "Percentage") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

##### LULVHRS
```{r}
LULVHRS_vals <- unique(linear_dat$LULVHRS)

# get the counts for each value for each cluster
LULVHRS_1 <- data.frame(table(cluster_1_rows$LULVHRS))
LULVHRS_2 <- data.frame(table(cluster_2_rows$LULVHRS))
LULVHRS_3 <- data.frame(table(cluster_3_rows$LULVHRS))

# add the cluster number to each dataframe
LULVHRS_1 <- mutate(LULVHRS_1, cluster = 1)
LULVHRS_2 <- mutate(LULVHRS_2, cluster = 2)
LULVHRS_3 <- mutate(LULVHRS_3, cluster = 3)

# Removing all negative values
LULVHRS_1 <- LULVHRS_1[-c(1:2), ]
LULVHRS_2 <- LULVHRS_2[-c(1:2), ]
LULVHRS_3 <- LULVHRS_3[-c(1:2), ]

# rename the columns
LULVHRS_1 <- rename(LULVHRS_1, LULVHRS = Var1, Freq = Freq)
LULVHRS_2 <- rename(LULVHRS_2, LULVHRS = Var1, Freq = Freq)
LULVHRS_3 <- rename(LULVHRS_3, LULVHRS = Var1, Freq = Freq)

# merge the dataframes
LULVHRS_dat <- rbind(LULVHRS_1, LULVHRS_2, LULVHRS_3)
LULVHRS_dat

# change cluster to factor
LULVHRS_dat$cluster <- as.factor(LULVHRS_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LULVHRS_dat, aes(x = LULVHRS, y = Freq, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Count of Each Value for LULVHRS for Each Cluster", x = "Cluster", y = "Count") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for LULVHRS for each cluster
# get the total number of rows for each cluster
LULVHRS_total_1 <- nrow(cluster_1_rows)
LULVHRS_total_2 <- nrow(cluster_2_rows)
LULVHRS_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LULVHRS_1 <- mutate(LULVHRS_1, percent = Freq / LULVHRS_total_1)
LULVHRS_2 <- mutate(LULVHRS_2, percent = Freq / LULVHRS_total_2)
LULVHRS_3 <- mutate(LULVHRS_3, percent = Freq / LULVHRS_total_3)

# printing the values to make sure they equal 1
sum(LULVHRS_1$percent)
sum(LULVHRS_2$percent)
sum(LULVHRS_3$percent)

# merge the dataframes
LULVHRS_dat <- rbind(LULVHRS_1, LULVHRS_2, LULVHRS_3)

# change cluster to factor
LULVHRS_dat$cluster <- as.factor(LULVHRS_dat$cluster)

# plot the data
ggplot(LULVHRS_dat, aes(x = LULVHRS, y = percent, fill = cluster)) +
    geom_bar(stat = "identity", position = "dodge", group = LULVHRS_dat$LULVHRS) +
    labs(title = "Percentage of Each Value for LULVHRS for Each Cluster", x = "Cluster", y = "Percentage") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Logisitic Regression - Part/Full-Time
### Set Up
```{r glm setup}
set.seed(0)

# pull columns from survey data
temp <- surveyq_dat %>% select(-yrs_of_collegecred, -how_hs_diploma, -missed_work, -left_last_job, -active_duty)

# remove duplicates
temp <- temp[!duplicated(temp), ]

# join data with summary data
temp <- left_join(summary_dat, temp, by = "id")
print("temp size:")
dim(temp)

# grab only rows containing Full and Part Time Individuals
logreg_dat <- temp[temp$employment_level == "(1) Full time" | temp$employment_level == "(2) Part time", ]

# cleaning log reg data before modeling
logreg_dat$employment_level <- factor(logreg_dat$employment_level)
logreg_dat <- logreg_dat[!duplicated(logreg_dat), ]
cols <- c("memory_issues", "dressing_issues", "hearing_issues", "eye_issues", "issues_errands", "issues_walking")

logreg_dat <- logreg_dat[logreg_dat[cols] != "(-1) Blank", ]
logreg_dat <- logreg_dat[logreg_dat$unionized != "(-1) Blank", ]

logreg_dat <- logreg_dat %>% select(-(T010101:T189999), -weekly_income)
logreg_dat <- logreg_dat[!duplicated(logreg_dat), ]
```

### Testing/Training Data
```{r glm testing training}
# training/testing data
summ_rows <- nrow(logreg_dat)
train_samp <- sample(1:summ_rows, summ_rows * .8)
train <- logreg_dat[train_samp, ] %>% select(-id)
test <- logreg_dat[-train_samp, ] %>% select(-id)

print("train size")
dim(train)
print("test size")
dim(test)
unique(logreg_dat$employment_level)
```
### GLM Model
```{r logreg model}
logreg <- glm(formula = employment_level ~ ., data = train, family = "binomial")

test_pred <- predict(logreg, test[, -6], type = "response")
test_pred <- ifelse(test_pred > .5, 2, 1)
actual <- test$employment_level
confmat <- table(test_pred, actual)
confmat
log_acc <- sum(diag(confmat)) / sum(confmat)
log_acc

full_time_misclass <- confmat[2, 1] / sum(confmat[, 1])
print(paste("Full Time Error Rate:", full_time_misclass))
part_time_misclass <- confmat[1, 2] / sum(confmat[, 2])
print(paste("Part Time Error Rate:", part_time_misclass))
print(paste("prop of full time in test:", sum(test$employment_level == "(1) Full time") / nrow(test)))
```

```{r}
plot(logreg)
```

```{r}
test_pred <- predict(logreg, test[, -6], type = "response")
test_pred <- ifelse(test_pred > .3, 2, 1)
confmat <- table(test_pred, actual)
confmat
log_acc <- sum(diag(confmat)) / sum(confmat)
log_acc

full_time_misclass <- confmat[2, 1] / sum(confmat[, 1])
print(paste("Full Time Error Rate:", full_time_misclass))
part_time_misclass <- confmat[1, 2] / sum(confmat[, 2])
print(paste("Part Time Error Rate:", part_time_misclass))
# print(paste("prop of full time in test:",sum(test$employment_level=="(1) Full time")/nrow(test)))
```


```{r}
xtrain <- as.matrix(train[, -6])
xtest <- as.matrix(test[, -6])
ytrain <- as.matrix(train[, 6])
ytest <- as.matrix(test[, 6])
ridge_reg <- glmnet(xtrain, ytrain, alpha = 1)

confusion.glmnet(ridge_reg, xtest, family = "binomial")
```



## Random Forest
### Set up
```{r}
# Remove all NA's and -1 from weekly_income
rf_dat <- summary_dat %>%
    filter(weekly_income != -0.01) %>%
    filter(!is.na(weekly_income))
```

### Random Forest Regression

```{r}
# Removing column 1 - 7 from rf_dat
rfr_dat <- rf_dat %>%
    select(-c(1:7))

# Creating the training and testing data sets
set.seed(456)
train_index <- sample(1:nrow(rfr_dat), size = 0.75 * nrow(rfr_dat))
rfr_train <- rfr_dat[train_index, ]
rfr_test <- rfr_dat[-train_index, ]
```

```{r}
# Creating the random forest classifier
rfr_model <- randomForest(weekly_income ~ ., data = rfr_train, ntree = 500)
rfr_model
```

```{r}
rfr_importance <- importance(rfr_model)
# order the importance
rfr_importance <- rfr_importance[order(rfr_importance[, 1], decreasing = TRUE), ]
rfr_importance
```


```{r}
# Predicting the income of the test data set
rfr_pred <- predict(rfr_model, rfr_test)
# rf_pred
```

```{r}
# Calculate MSE
rfr_mse <- mean((rfr_test$weekly_income - rfr_pred)^2)
rfr_mse
rfr_rmse <- sqrt(rfr_mse)
rfr_rmse

# rfr_test
rfr_sst <- sum((rfr_test$weekly_income - mean(rfr_test$weekly_income))^2)
rfr_ssr <- sum((rfr_pred - mean(rfr_test$weekly_income))^2)
# Calculate R^2
rfr_r2 <- 1 - (rfr_ssr / rfr_sst)
rfr_r2
```

```{r}
sampl_num <- 250
# sampling sampl_num random rows from the test data set
# and plotting the predicted weekly income vs the actual weekly income
rfr_sample <- rfr_test[sample(1:nrow(rfr_test), sampl_num), ]
rfr_sample <- rfr_sample %>%
    mutate(pred = rfr_pred[1:sampl_num])
rfr_sample

# finds the max to set bounds for the plot and increases it by a bit
sampl_max <- max(rfr_sample$weekly_income, rfr_sample$pred) * 1.025
```

```{r, fig.width=6,fig.height=6}
# Plotting the predicted weekly income vs the actual weekly income
ggplot(rfr_sample, aes(x = weekly_income, y = pred)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    labs(
        title = "QQ - Predicted Weekly Income vs Actual Weekly Income",
        x = "Actual Weekly Income", y = "Predicted Weekly Income"
    ) +
    xlim(c(0, sampl_max)) +
    ylim(c(0, sampl_max))
```

### Lasso Regression

```{r}
# Using same training and testing data sets as random forest regression
# Creating the lasso random forest classifer with ranger
lasso_model <- ranger(weekly_income ~ .,
    data = rfr_train, num.trees = 500,
    importance = "impurity", verbose = TRUE
)
```

```{r}
# Predicting the income of the test data set
lasso_pred <- predict(lasso_model, rfr_test)$predictions
# lasso_pred
```

```{r}
# Calculate MSE
lasso_mse <- mean((rfr_test$weekly_income - lasso_pred)^2)
lasso_mse
lasso_rmse <- sqrt(lasso_mse)
lasso_rmse

# rfr_test
lasso_sst <- sum((rfr_test$weekly_income - mean(rfr_test$weekly_income))^2)
lasso_ssr <- sum((lasso_pred - mean(rfr_test$weekly_income))^2)
# Calculate R^2
lasso_r2 <- 1 - (lasso_ssr / lasso_sst)
lasso_r2
```

```{r}
# importance of each variable
lasso_importance <- lasso_model$variable.importance
# c('double', 'numeric')
# save to csv to sort in python cause I can't figure out how to do it in R
write.csv(lasso_importance, "python/lasso_importance.csv")
```

```{r}
# read in the sorted csv
lasso_importance <- read.csv("python/lasso_importance_sorted.csv")
lasso_importance
```

```{r}
lasso_importance_25 <- lasso_importance[1:20, ]
# plotting the importance of each variable
ggplot(lasso_importance_25, aes(x = reorder(feature, importance), y = importance)) +
    geom_bar(stat = "identity") +
    labs(
        title = "Lasso - Importance of Each Variable",
        x = "Variable", y = "Importance"
    ) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## RF Classifier

```{r random forest classifier creating column}
# Creating income brackets
# Dividing weekly_income into 5 brackets
brackets <- quantile(rf_dat$weekly_income, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))
brackets
```

```{r}
# Add income bracket to rf_dat
rfc_dat <- rf_dat %>%
    mutate(income_bracket = case_when(
        weekly_income <= brackets[2] ~ "very low",
        weekly_income > brackets[2] & weekly_income <= brackets[3] ~ "low",
        weekly_income > brackets[3] & weekly_income <= brackets[4] ~ "medium",
        weekly_income > brackets[4] & weekly_income <= brackets[5] ~ "high",
        weekly_income > brackets[5] ~ "very high"
    ))
# Moving income_bracket to the front of the data frame
rfc_dat <- rfc_dat %>%
    select(income_bracket, everything())
# convert income_bracket to factor
rfc_dat$income_bracket <- as.factor(rfc_dat$income_bracket)
# Order the factor levels for income
rfc_dat$income_bracket <- ordered(rfc_dat$income_bracket,
    levels =
        c("very low", "low", "medium", "high", "very high")
)
# removing columns 2-8
rfc_dat <- rfc_dat %>%
    select(-c(2:8))
```

```{r}
# Print the income bracket types and their factor as well as their counts
table(rfc_dat$income_bracket)
# print total number of observations
nrow(rfc_dat)
# find the total number of variables
ncol(rfc_dat) - 1
```

```{r}
# Remove weekly_income from rfc_dat
rfc_dat <- rfc_dat %>%
    select(-weekly_income)
```

```{r}
# Creating the training and testing data sets
# set.seed(456)
train_index <- sample(1:nrow(rfc_dat), size = 0.75 * nrow(rfc_dat))
rfc_train <- rfc_dat[train_index, ]
rfc_test <- rfc_dat[-train_index, ]
```

```{r}
# Creating the random forest classifier
rfc_model <- randomForest(income_bracket ~ ., data = rfc_train, ntree = 500)
# rfc_model
```

```{r}
rfc_importance <- importance(rfc_model)
# order the importance
rfc_importance <- rfc_importance[order(rfc_importance[, 1], decreasing = TRUE), ]
rfc_importance
```

```{r}
# getting the values from the model
rfc_model$confusion
```

```{r}
plot(rfc_model)
# you can find out which line is which by matching the error rate at the end
# with the classification error
```

```{r}
# Predicting the income bracket of the test data set
rfc_pred <- predict(rfc_model, rfc_test)
# rfc_pred
```

```{r}
# import confusionMatrix
library(caret)
```

```{r}
# Confusion matrix
confusionMatrix(rfc_pred, rfc_test$income_bracket)
```

```{r}
rfc_model$confusion
```

```{r}
# find most important features and make plot bigger
varImpPlot(rfc_model, main = "Random Forest Classifier", n.var = 10)
```

```{r}
# Find the average values of each feature for each income bracket
rfc_dat_avg <- rfc_dat %>%
    group_by(income_bracket) %>%
    summarise_all(mean)
rfc_dat_avg
```

```{r}
# export to csv to perform row analysis in python
# since I can't figure it out in R
write.csv(rfc_dat_avg, "rfc_dat_avg.csv")
```

## Logistic Regression
```{r Log Reg setup}
# PRFTLF
# Reading in the 4th file
load(file = "34453-0004-Data.rda")
employ_dat_orig <- da34453.0004

employ_dat <- employ_dat_orig %>%
    select(TUCASEID, PRFTLF)



activity_dat <- activity_dat %>%
    select(
        TUCASEID, TUACT_N, TEWHERE, TUACTDUR,
        TUT1CODE, TUT2CODE, TUT3CODE, TRCODE
    )

summary_dat <- summary_dat %>%
    select(
        TUCASEID, TEAGE, TESEX, PEEDUCA, GTMETSTA,
        TELFS, TRDPFTPT, TRERNWA, T010101:T189999
    )

head(activity_dat)
head(summary_dat)
```