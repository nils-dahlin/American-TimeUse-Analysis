---
title: "Stat 288 Time Use Analysis"
author: "Nils Dahlin, Gian Cercena"
date: "4/17/2023"
output: html_document
---

# Examining the Time Use of Americans and Predicting Personal Features 


## Data Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries needed
library(dplyr)
library(rpart)
library(tree)
library(ggplot2)
library(tidyr)
library(randomForest)
library(glmnet)
library(ranger)

# activity_dat contains the data/features regarding the
# activities respondents did throughout the week as well
# as related variables
load(file = "34453-0001-Data.rda")
activity_dat_orig <- da34453.0001

# summary_dat contains the data/features regarding the
# individual repsondents, contains features like:
# part/full time status, income, job type, family stats, etc.
load(file = "34453-0008-Data.rda")
summary_dat_orig <- da34453.0008

# surveyq_dat contains responses from individuals to survey questions
# includes questions like: Years of college credit completed?
# Have you looked for work in the past year? How did you get your
# highschool diploma?
load(file = "34453-0004-Data.rda")
surveyq_dat_orig <- da34453.0004

# linear_dat contains the features used in the linear regression
load(file = "34453-0011-Data.rda")
linear_dat_orig <- da34453.0011
```


### Pulling Features We Need/Looking at the Raw Data
```{r reducing dataset}
activity_dat <- activity_dat_orig %>%
    select(
        TUCASEID, TUACT_N, TEWHERE, TUACTDUR,
        TUT1CODE, TUT2CODE, TUT3CODE, TRCODE
    )

summary_dat <- summary_dat_orig %>%
    select(
        TUCASEID, TEAGE, TESEX, PEEDUCA, GTMETSTA,
        TELFS, TRDPFTPT, TRERNWA, T010101:T189999
    )

surveyq_dat <- surveyq_dat_orig %>%
    select(
        TUCASEID, HEHOUSUT, HETENURE, HRNUMHOU, 
        PEABSRSN, PECYC, PEJHRSN, PEDIPGED, PEAFEVER,
        PEDISREM, PEDISDRS, PEDISEAR, PEDISEYE, PEDISOUT,
        PEDISPHY, PEERNLAB
    )

linear_dat <- linear_dat_orig %>%
    select(
        TUCASEID,
        LEGNHTH, LEPAIN, LRADJ, LUADDY, LUADHR, 
        LUADLOC, LULEAVE, LULVHRS, LULVYTD, LUPAID, 
        LUPDBRTH, LUPDCC, LUPDEC, LUPDERR, LUPDFMIL, 
        LUPDOIL, LUPDVAC, LUPTHOL, LUPTO, LUPTOHOL, 
        LUPTOMAT, LUPTOSCK, LUPTPSL, LUPTSCK, LUPTVAC
    )

head(activity_dat)
head(summary_dat)
head(surveyq_dat)
head(linear_dat)
```

### Renaming Certain Features/Columns
If this code chunk has errors, rerun one above
```{r renaming features}
# in activity_dat the three TUTCODE's are the codes for the
# primary, secondary, and tertiary activity codes which combined
# make the 6 digit activity code for each specific activity measured
# see here https://www.bls.gov/tus/lexicons/lexiconnoex0321.pdf)
activity_dat <- activity_dat %>%
    rename(
        "id" = "TUCASEID",
        "activity_num" = "TUACT_N",
        "location" = "TEWHERE",
        "duration" = "TUACTDUR",
        "code1" = "TUT1CODE",
        "code2" = "TUT2CODE",
        "code3" = "TUT3CODE",
        "activity_code" = "TRCODE"
    )
head(activity_dat)

# renaming non-activity code features
summary_dat <- summary_dat %>%
    rename(
        "id" = "TUCASEID",
        "age" = "TEAGE",
        "sex" = "TESEX",
        "education" = "PEEDUCA",
        "metro_area" = "GTMETSTA",
        "employment_status" = "TELFS",
        "employment_level" = "TRDPFTPT",
        "weekly_income" = "TRERNWA"
    )
head(summary_dat)

surveyq_dat <- surveyq_dat %>%
    rename(
        "id" = "TUCASEID",
        "type_of_housing" = "HEHOUSUT",
        "ownership_of_home" = "HETENURE",
        "num_in_household" = "HRNUMHOU",
        "missed_work" = "PEABSRSN",
        "yrs_of_collegecred" = "PECYC",
        "how_hs_diploma" = "PEDIPGED",
        "left_last_job" = "PEJHRSN",
        "active_duty" = "PEAFEVER",
        "memory_issues" = "PEDISREM",
        "dressing_issues" = "PEDISDRS",
        "hearing_issues" = "PEDISEAR",
        "eye_issues" = "PEDISEYE",
        "issues_errands" = "PEDISOUT",
        "issues_walking" = "PEDISPHY",
        "unionized" = "PEERNLAB"
    )
head(surveyq_dat)

linear_dat <- linear_dat %>%
    rename(
        "id" = "TUCASEID",
    )

# Add two decimal places to weekly_income
summary_dat$weekly_income <- summary_dat$weekly_income / 100
```
### Surveyq Test
```{r testing dat}
dim(surveyq_dat)
surveyq_dat[duplicated(surveyq_dat$TUCASEID),]

surveyq_dat[]
```


### Combining Datasets
need to remove duplicates from surveyq_dat due to yrs_of_collegecred (keep non blank entries for duplicate id's)
```{r combine data}
summary_dat <- summary_dat[!duplicated(summary_dat), ]
surveyq_dat <- surveyq_dat[!duplicated(surveyq_dat), ]
dim(summary_dat)
dim(surveyq_dat)
```


## Linear Regression
### Set Up
```{r data set up}
# Want to find if flexibility in jobs is correlated with
# the income of the individual

# getting all salary data from summary_dat
salary_dat <- summary_dat %>%
    select(id, weekly_income)

# matching the salary ids and the linear_dat ids
linear_dat <- left_join(linear_dat, salary_dat, by = "id")
dim(linear_dat)

linear_dat <- linear_dat %>%
  select(-id)

# linear_dat$weekly_income <- linear_dat$weekly_income / 100

# removing rows with no income data
linear_dat <- linear_dat[!is.na(linear_dat$weekly_income), ]
dim(linear_dat)

head(linear_dat)
```

```{r linear train test}
# training/testing data
set.seed(47)
lin_train <- sample(1:nrow(linear_dat), 0.8 * nrow(linear_dat))
lin_test <- setdiff(1:nrow(linear_dat), lin_train)

# making training and testing data
lin_train_dat <- linear_dat[lin_train, ]
lin_test_dat <- linear_dat[lin_test, ]
```

### Statistics About the Dataset
```{r}
# average weekly income across entire dataset, as well as
# sd
mean(linear_dat$weekly_income)
median(linear_dat$weekly_income)
sd(linear_dat$weekly_income)
```

```{r}
# density plot of weekly income
plot(density(linear_dat$weekly_income))
```

### Making the Model
```{r}
# making the linear model
head(lin_train_dat)
lm1 <- lm(weekly_income ~ ., data = lin_train_dat)
summary(lm1)
```

```{r}
# testing the model
pred <- predict(lm1, lin_test_dat)
mse <- mean((pred - lin_test_dat$weekly_income)^2)
mse
sqrt(mse)
```

### PCA
```{r pca}
# turn all data into numeric
lin_pca <- as.data.frame(lapply(linear_dat, as.numeric))
head(lin_pca)
# run pca over linear_dat to 2 dimensions
pca <- prcomp(lin_pca, scale = TRUE)
summary(pca)
```

```{r}
# graphing cumulative variance
plot(pca, type = "l")
```

```{r}
# plotting two dimensions
plot(pca$x[,1], pca$x[,2])
```

### Cluster Analysis on PCA
```{r}
# run cluster analysis on pca
set.seed(74)
kmeans_pca <- kmeans(pca$x, centers = 3)
kmeans_pca
```

```{r}
# red yellow green
colors <- c("red", "orange", "green") 
# plot the clusters with labels for which cluster they are
plot(pca$x[,1], pca$x[,2], col = colors[kmeans_pca$cluster])
# putting the labels on the center of the clusters
text(kmeans_pca$centers[,1], kmeans_pca$centers[,2], labels = 1:3, col = "black")
```


```{r}
# This and the following code block were originally meant to analyze some data
# but it wasn't meaningful because the data categories weren't ordinal
# The dataset avg_dat_graph is needed though, so these blocks still need to
# be run.

# get the average values in the linear data for each cluster
cluster1 <- linear_dat[kmeans_pca$cluster == 1, ]
cluster2 <- linear_dat[kmeans_pca$cluster == 2, ]
cluster3 <- linear_dat[kmeans_pca$cluster == 3, ]

# convert the cluster columnsn to numeric
cluster1 <- as.data.frame(lapply(cluster1, as.numeric))
cluster2 <- as.data.frame(lapply(cluster2, as.numeric))
cluster3 <- as.data.frame(lapply(cluster3, as.numeric))

# get the average values for each cluster
avg1 <- colMeans(cluster1)
avg2 <- colMeans(cluster2)
avg3 <- colMeans(cluster3)

# combine the averages into a dataframe
avg_dat <- data.frame(avg1, avg2, avg3)

# transpose the dataframe
avg_dat <- t(avg_dat)

# rename the columns
rownames(avg_dat) <- c("cluster1", "cluster2", "cluster3")

avg_dat

```

```{r}
# convert the avg_dat to a dataframe
avg_dat_graph <- as.data.frame(avg_dat)

# add a column for the cluster
avg_dat_graph$cluster <- rownames(avg_dat_graph)

# convert the dataframe to long format
avg_dat_graph <- gather(avg_dat_graph, variable, value, -cluster)

# ggplot(avg_dat_graph, aes(x = variable, y = value, fill = cluster)) +
#   geom_bar(stat = "identity", position = "dodge") +
#       labs(title = "Average Values of Variables for Each Cluster", x = "Variables", y = "Average Value") +
#     theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# get the average salary for each cluster
avg_sal <- linear_dat %>%
  group_by(kmeans_pca$cluster) %>%
  summarise(avg_sal = mean(weekly_income))

# plot the average salary for each cluster with the colors
ggplot(avg_dat_graph, aes(x = cluster, y = value, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
      labs(title = "Average Salary for Each Cluster", x = "Cluster", y = "Average Salary") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r}
# get average salary across entire dataset
avg_sal_all <- mean(linear_dat$weekly_income)
avg_sal_all
```

### Cluster Plotting
```{r}
cluster_1_rows <- linear_dat[kmeans_pca$cluster == 1, ]
cluster_2_rows <- linear_dat[kmeans_pca$cluster == 2, ]
cluster_3_rows <- linear_dat[kmeans_pca$cluster == 3, ]

linear_dat
```
#### LEPAIN
```{r}
LEPAIN_vals <- unique(linear_dat$LEPAIN)

# get the counts for each value for each cluster
LEPAIN_1 <- data.frame(table(cluster_1_rows$LEPAIN))
LEPAIN_2 <- data.frame(table(cluster_2_rows$LEPAIN))
LEPAIN_3 <- data.frame(table(cluster_3_rows$LEPAIN))

# add the cluster number to each dataframe
LEPAIN_1 <- mutate(LEPAIN_1, cluster = 1)
LEPAIN_2 <- mutate(LEPAIN_2, cluster = 2)
LEPAIN_3 <- mutate(LEPAIN_3, cluster = 3)

# rename the columns
LEPAIN_1 <- rename(LEPAIN_1, LEPAIN = Var1, Freq = Freq)
LEPAIN_2 <- rename(LEPAIN_2, LEPAIN = Var1, Freq = Freq)
LEPAIN_3 <- rename(LEPAIN_3, LEPAIN = Var1, Freq = Freq)

# merge the dataframes
LEPAIN_dat <- rbind(LEPAIN_1, LEPAIN_2, LEPAIN_3)
LEPAIN_dat

# change cluster to factor
LEPAIN_dat$cluster <- as.factor(LEPAIN_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LEPAIN_dat, aes(x = LEPAIN, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LEPAIN for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for lepain for each cluster
# get the total number of rows for each cluster
LEPAIN_total_1 <- nrow(cluster_1_rows)
LEPAIN_total_2 <- nrow(cluster_2_rows)
LEPAIN_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LEPAIN_1 <- mutate(LEPAIN_1, percent = Freq / LEPAIN_total_1)
LEPAIN_2 <- mutate(LEPAIN_2, percent = Freq / LEPAIN_total_2)
LEPAIN_3 <- mutate(LEPAIN_3, percent = Freq / LEPAIN_total_3)

# printing the values to make sure they equal 1
sum(LEPAIN_1$percent)
sum(LEPAIN_2$percent)
sum(LEPAIN_3$percent)

# merge the dataframes
LEPAIN_dat <- rbind(LEPAIN_1, LEPAIN_2, LEPAIN_3)

# change cluster to factor
LEPAIN_dat$cluster <- as.factor(LEPAIN_dat$cluster)

# plot the data
ggplot(LEPAIN_dat, aes(x = LEPAIN, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LEPAIN_dat$LEPAIN) +
  labs(title = "Percentage of Each Value for LEPAIN for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### LRADJ
```{r}
LRADJ_vals <- unique(linear_dat$LRADJ)

# get the counts for each value for each cluster
LRADJ_1 <- data.frame(table(cluster_1_rows$LRADJ))
LRADJ_2 <- data.frame(table(cluster_2_rows$LRADJ))
LRADJ_3 <- data.frame(table(cluster_3_rows$LRADJ))

# add the cluster number to each dataframe
LRADJ_1 <- mutate(LRADJ_1, cluster = 1)
LRADJ_2 <- mutate(LRADJ_2, cluster = 2)
LRADJ_3 <- mutate(LRADJ_3, cluster = 3)

# rename the columns
LRADJ_1 <- rename(LRADJ_1, LRADJ = Var1, Freq = Freq)
LRADJ_2 <- rename(LRADJ_2, LRADJ = Var1, Freq = Freq)
LRADJ_3 <- rename(LRADJ_3, LRADJ = Var1, Freq = Freq)

# merge the dataframes
LRADJ_dat <- rbind(LRADJ_1, LRADJ_2, LRADJ_3)
LRADJ_dat

# change cluster to factor
LRADJ_dat$cluster <- as.factor(LRADJ_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LRADJ_dat, aes(x = LRADJ, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LRADJ for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for LRADJ for each cluster
# get the total number of rows for each cluster
LRADJ_total_1 <- nrow(cluster_1_rows)
LRADJ_total_2 <- nrow(cluster_2_rows)
LRADJ_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LRADJ_1 <- mutate(LRADJ_1, percent = Freq / LRADJ_total_1)
LRADJ_2 <- mutate(LRADJ_2, percent = Freq / LRADJ_total_2)
LRADJ_3 <- mutate(LRADJ_3, percent = Freq / LRADJ_total_3)

# printing the values to make sure they equal 1
sum(LRADJ_1$percent)
sum(LRADJ_2$percent)
sum(LRADJ_3$percent)

# merge the dataframes
LRADJ_dat <- rbind(LRADJ_1, LRADJ_2, LRADJ_3)

# change cluster to factor
LRADJ_dat$cluster <- as.factor(LRADJ_dat$cluster)

# plot the data
ggplot(LRADJ_dat, aes(x = LRADJ, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LRADJ_dat$LRADJ) +
  labs(title = "Percentage of Each Value for LRADJ for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### LUADDY
```{r}
LUADDY_vals <- unique(linear_dat$LUADDY)

# get the counts for each value for each cluster
LUADDY_1 <- data.frame(table(cluster_1_rows$LUADDY))
LUADDY_2 <- data.frame(table(cluster_2_rows$LUADDY))
LUADDY_3 <- data.frame(table(cluster_3_rows$LUADDY))

# add the cluster number to each dataframe
LUADDY_1 <- mutate(LUADDY_1, cluster = 1)
LUADDY_2 <- mutate(LUADDY_2, cluster = 2)
LUADDY_3 <- mutate(LUADDY_3, cluster = 3)

# rename the columns
LUADDY_1 <- rename(LUADDY_1, LUADDY = Var1, Freq = Freq)
LUADDY_2 <- rename(LUADDY_2, LUADDY = Var1, Freq = Freq)
LUADDY_3 <- rename(LUADDY_3, LUADDY = Var1, Freq = Freq)

# merge the dataframes
LUADDY_dat <- rbind(LUADDY_1, LUADDY_2, LUADDY_3)
LUADDY_dat

# change cluster to factor
LUADDY_dat$cluster <- as.factor(LUADDY_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LUADDY_dat, aes(x = LUADDY, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LUADDY for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for LUADDY for each cluster
# get the total number of rows for each cluster
LUADDY_total_1 <- nrow(cluster_1_rows)
LUADDY_total_2 <- nrow(cluster_2_rows)
LUADDY_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LUADDY_1 <- mutate(LUADDY_1, percent = Freq / LUADDY_total_1)
LUADDY_2 <- mutate(LUADDY_2, percent = Freq / LUADDY_total_2)
LUADDY_3 <- mutate(LUADDY_3, percent = Freq / LUADDY_total_3)

# printing the values to make sure they equal 1
sum(LUADDY_1$percent)
sum(LUADDY_2$percent)
sum(LUADDY_3$percent)

# merge the dataframes
LUADDY_dat <- rbind(LUADDY_1, LUADDY_2, LUADDY_3)

# change cluster to factor
LUADDY_dat$cluster <- as.factor(LUADDY_dat$cluster)

# plot the data
ggplot(LUADDY_dat, aes(x = LUADDY, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LUADDY_dat$LUADDY) +
  labs(title = "Percentage of Each Value for LUADDY for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### LULVHRS
```{r}
LULVHRS_vals <- unique(linear_dat$LULVHRS)

# get the counts for each value for each cluster
LULVHRS_1 <- data.frame(table(cluster_1_rows$LULVHRS))
LULVHRS_2 <- data.frame(table(cluster_2_rows$LULVHRS))
LULVHRS_3 <- data.frame(table(cluster_3_rows$LULVHRS))

# add the cluster number to each dataframe
LULVHRS_1 <- mutate(LULVHRS_1, cluster = 1)
LULVHRS_2 <- mutate(LULVHRS_2, cluster = 2)
LULVHRS_3 <- mutate(LULVHRS_3, cluster = 3)

# Removing all negative values
LULVHRS_1 <- LULVHRS_1[-c(1:2),]
LULVHRS_2 <- LULVHRS_2[-c(1:2),]
LULVHRS_3 <- LULVHRS_3[-c(1:2),]

# rename the columns
LULVHRS_1 <- rename(LULVHRS_1, LULVHRS = Var1, Freq = Freq)
LULVHRS_2 <- rename(LULVHRS_2, LULVHRS = Var1, Freq = Freq)
LULVHRS_3 <- rename(LULVHRS_3, LULVHRS = Var1, Freq = Freq)

# merge the dataframes
LULVHRS_dat <- rbind(LULVHRS_1, LULVHRS_2, LULVHRS_3)
LULVHRS_dat

# change cluster to factor
LULVHRS_dat$cluster <- as.factor(LULVHRS_dat$cluster)

# plot adjacent bars for each value for each cluster
ggplot(LULVHRS_dat, aes(x = LULVHRS, y = Freq, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Count of Each Value for LULVHRS for Each Cluster", x = "Cluster", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# plotting the percentage of each value for LULVHRS for each cluster
# get the total number of rows for each cluster
LULVHRS_total_1 <- nrow(cluster_1_rows)
LULVHRS_total_2 <- nrow(cluster_2_rows)
LULVHRS_total_3 <- nrow(cluster_3_rows)

# get the percentage of each value for each cluster
LULVHRS_1 <- mutate(LULVHRS_1, percent = Freq / LULVHRS_total_1)
LULVHRS_2 <- mutate(LULVHRS_2, percent = Freq / LULVHRS_total_2)
LULVHRS_3 <- mutate(LULVHRS_3, percent = Freq / LULVHRS_total_3)

# printing the values to make sure they equal 1
sum(LULVHRS_1$percent)
sum(LULVHRS_2$percent)
sum(LULVHRS_3$percent)

# merge the dataframes
LULVHRS_dat <- rbind(LULVHRS_1, LULVHRS_2, LULVHRS_3)

# change cluster to factor
LULVHRS_dat$cluster <- as.factor(LULVHRS_dat$cluster)

# plot the data
ggplot(LULVHRS_dat, aes(x = LULVHRS, y = percent, fill = cluster)) +
  geom_bar(stat = "identity", position = "dodge", group = LULVHRS_dat$LULVHRS) +
  labs(title = "Percentage of Each Value for LULVHRS for Each Cluster", x = "Cluster", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
#### 

## Logisitic Regression - Part/Full-Time
### Set Up
```{r glm setup}
set.seed(0)

# pull columns from survey data
temp <- surveyq_dat %>% select(-yrs_of_collegecred,-how_hs_diploma, -missed_work, -left_last_job, -active_duty)

# remove duplicates
temp <- temp[!duplicated(temp), ]

# join data with summary data
temp <- left_join(summary_dat, temp, by = "id")
print("temp size:")
dim(temp)

# grab only rows containing Full and Part Time Individuals
logreg_dat = temp[temp$employment_level=="(1) Full time"|temp$employment_level=="(2) Part time",]

# cleaning log reg data before modeling
logreg_dat$employment_level = factor(logreg_dat$employment_level)
logreg_dat <- logreg_dat[!duplicated(logreg_dat),]
cols = c("memory_issues","dressing_issues","hearing_issues","eye_issues","issues_errands","issues_walking")

logreg_dat <- logreg_dat[logreg_dat[cols]!="(-1) Blank",] 
logreg_dat <- logreg_dat[logreg_dat$unionized!="(-1) Blank",]

logreg_dat = logreg_dat %>% select(-(T010101:T189999),-weekly_income)
logreg_dat <- logreg_dat[!duplicated(logreg_dat),]
```

### Testing/Training Data
```{r glm testing training}
# training/testing data
summ_rows <- nrow(logreg_dat)
train_samp <- sample(1:summ_rows, summ_rows * .8)
train <- logreg_dat[train_samp, ] %>% select(-id)
test <- logreg_dat[-train_samp, ] %>% select(-id)

print("train size");dim(train)
print("test size");dim(test)
unique(logreg_dat$employment_level)
```
### GLM Model
```{r logreg model}
logreg <- glm(formula=employment_level~., data = train, family = "binomial")

test_pred = predict(logreg, test[,-6],type = "response")
test_pred = ifelse(test_pred>.5,2,1)
actual = test$employment_level
confmat = table(test_pred,actual)
confmat
log_acc = sum(diag(confmat))/sum(confmat)
log_acc

full_time_misclass = confmat[2,1]/sum(confmat[,1])
print(paste("Full Time Error Rate:",full_time_misclass))
part_time_misclass = confmat[1,2]/sum(confmat[,2])
print(paste("Part Time Error Rate:",part_time_misclass))
print(paste("prop of full time in test:",sum(test$employment_level=="(1) Full time")/nrow(test)))
```

```{r}
plot(logreg)
```

```{r}
test_pred = predict(logreg, test[,-6],type = "response")
test_pred = ifelse(test_pred>.3,2,1)
confmat = table(test_pred,actual)
confmat
log_acc = sum(diag(confmat))/sum(confmat)
log_acc

full_time_misclass = confmat[2,1]/sum(confmat[,1])
print(paste("Full Time Error Rate:",full_time_misclass))
part_time_misclass = confmat[1,2]/sum(confmat[,2])
print(paste("Part Time Error Rate:",part_time_misclass))
#print(paste("prop of full time in test:",sum(test$employment_level=="(1) Full time")/nrow(test)))
```


```{r}
xtrain = as.matrix(train[,-6])
xtest = as.matrix(test[,-6])
ytrain = as.matrix(train[,6])
ytest = as.matrix(test[,6])
ridge_reg = glmnet(xtrain, ytrain,  alpha = 1)

confusion.glmnet(ridge_reg,xtest,family = "binomial")

```



## Random Forest
### Set up
```{r}
# Remove all NA's and -1 from weekly_income
rf_dat <- summary_dat %>%
    filter(weekly_income != -0.01) %>%
    filter(!is.na(weekly_income))
```

### Random Forest Regression

```{r}
# Removing column 1 - 7 from rf_dat
rfr_dat <- rf_dat %>%
    select(-c(1:7))

# Creating the training and testing data sets
set.seed(456)
train_index <- sample(1:nrow(rfr_dat), size = 0.75 * nrow(rfr_dat))
rfr_train <- rfr_dat[train_index, ]
rfr_test <- rfr_dat[-train_index, ]
```

```{r}
# Creating the random forest classifier
rfr_model <- randomForest(weekly_income ~ ., data = rfr_train, ntree = 500)
rfr_model
```

```{r}
rfr_importance <- importance(rfr_model)
# order the importance
rfr_importance <- rfr_importance[order(rfr_importance[, 1], decreasing = TRUE), ]
rfr_importance
```


```{r}
# Predicting the income of the test data set
rfr_pred <- predict(rfr_model, rfr_test)
# rf_pred
```

```{r}
# Calculate MSE
rfr_mse <- mean((rfr_test$weekly_income - rfr_pred)^2)
rfr_mse
rfr_rmse <- sqrt(rfr_mse)
rfr_rmse

# rfr_test
rfr_sst <- sum((rfr_test$weekly_income - mean(rfr_test$weekly_income))^2)
rfr_ssr <- sum((rfr_pred - mean(rfr_test$weekly_income))^2)
# Calculate R^2
rfr_r2 <- 1 - (rfr_ssr / rfr_sst)
rfr_r2
```

```{r}
sampl_num <- 250
# sampling sampl_num random rows from the test data set
# and plotting the predicted weekly income vs the actual weekly income
rfr_sample <- rfr_test[sample(1:nrow(rfr_test), sampl_num), ]
rfr_sample <- rfr_sample %>%
    mutate(pred = rfr_pred[1:sampl_num])
rfr_sample

# finds the max to set bounds for the plot and increases it by a bit
sampl_max <- max(rfr_sample$weekly_income, rfr_sample$pred) * 1.025
```

```{r, fig.width=6,fig.height=6}
# Plotting the predicted weekly income vs the actual weekly income
ggplot(rfr_sample, aes(x = weekly_income, y = pred)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    labs(title = "QQ - Predicted Weekly Income vs Actual Weekly Income", 
         x = "Actual Weekly Income", y = "Predicted Weekly Income") +
    xlim(c(0, sampl_max)) +
    ylim(c(0, sampl_max))
```

### Lasso Regression

```{r}
# Using same training and testing data sets as random forest regression
# Creating the lasso random forest classifer with ranger
lasso_model <- ranger(weekly_income ~ ., data = rfr_train, num.trees = 500, 
                      importance = "impurity", verbose = TRUE)
```

```{r}
# Predicting the income of the test data set
lasso_pred <- predict(lasso_model, rfr_test)$predictions
# lasso_pred
```

```{r}
# Calculate MSE
lasso_mse <- mean((rfr_test$weekly_income - lasso_pred)^2)
lasso_mse
lasso_rmse <- sqrt(lasso_mse)
lasso_rmse

# rfr_test
lasso_sst <- sum((rfr_test$weekly_income - mean(rfr_test$weekly_income))^2)
lasso_ssr <- sum((lasso_pred - mean(rfr_test$weekly_income))^2)
# Calculate R^2
lasso_r2 <- 1 - (lasso_ssr / lasso_sst)
lasso_r2
```

```{r}
# importance of each variable
lasso_importance <- lasso_model$variable.importance
# c('double', 'numeric')
# save to csv to sort in python cause I can't figure out how to do it in R
write.csv(lasso_importance, "python/lasso_importance.csv")
```

```{r}
# read in the sorted csv
lasso_importance <- read.csv("python/lasso_importance_sorted.csv")
lasso_importance
```

```{r}
lasso_importance_25 = lasso_importance[1:20, ]
# plotting the importance of each variable
ggplot(lasso_importance_25, aes(x = reorder(feature, importance), y = importance)) +
    geom_bar(stat = "identity") +
    labs(title = "Lasso - Importance of Each Variable", 
         x = "Variable", y = "Importance") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## RF Classifier

```{r random forest classifier creating column}
# Creating income brackets
# Dividing weekly_income into 5 brackets
brackets <- quantile(rf_dat$weekly_income, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))
brackets
```

```{r}
# Add income bracket to rf_dat
rfc_dat <- rf_dat %>%
    mutate(income_bracket = case_when(
        weekly_income <= brackets[2] ~ "very low",
        weekly_income > brackets[2] & weekly_income <= brackets[3] ~ "low",
        weekly_income > brackets[3] & weekly_income <= brackets[4] ~ "medium",
        weekly_income > brackets[4] & weekly_income <= brackets[5] ~ "high",
        weekly_income > brackets[5] ~ "very high"
    ))
# Moving income_bracket to the front of the data frame
rfc_dat <- rfc_dat %>%
    select(income_bracket, everything())
# convert income_bracket to factor
rfc_dat$income_bracket <- as.factor(rfc_dat$income_bracket)
# Order the factor levels for income
rfc_dat$income_bracket <- ordered(rfc_dat$income_bracket,
    levels =
        c("very low", "low", "medium", "high", "very high")
)
# removing columns 2-8
rfc_dat <- rfc_dat %>%
    select(-c(2:8))
```

```{r}
# Print the income bracket types and their factor as well as their counts
table(rfc_dat$income_bracket)
# print total number of observations
nrow(rfc_dat)
# find the total number of variables
ncol(rfc_dat) - 1
```

```{r}
# Remove weekly_income from rfc_dat
rfc_dat <- rfc_dat %>%
    select(-weekly_income)
```

```{r}
# Creating the training and testing data sets
# set.seed(456)
train_index <- sample(1:nrow(rfc_dat), size = 0.75 * nrow(rfc_dat))
rfc_train <- rfc_dat[train_index, ]
rfc_test <- rfc_dat[-train_index, ]
```

```{r}
# Creating the random forest classifier
rfc_model <- randomForest(income_bracket ~ ., data = rfc_train, ntree = 500)
# rfc_model
```

```{r}
rfc_importance <- importance(rfc_model)
# order the importance
rfc_importance <- rfc_importance[order(rfc_importance[, 1], decreasing = TRUE), ]
rfc_importance
```

```{r}
# getting the values from the model
rfc_model$confusion
```

```{r}
plot(rfc_model)
# you can find out which line is which by matching the error rate at the end
# with the classification error
```

```{r}
# Predicting the income bracket of the test data set
rfc_pred <- predict(rfc_model, rfc_test)
# rfc_pred
```

```{r}
# import confusionMatrix
library(caret)
```

```{r}
# Confusion matrix
confusionMatrix(rfc_pred, rfc_test$income_bracket)
```

```{r}
rfc_model$confusion
```

```{r}
# find most important features and make plot bigger
varImpPlot(rfc_model, main = "Random Forest Classifier", n.var = 10)
```

```{r}
# Find the average values of each feature for each income bracket
rfc_dat_avg <- rfc_dat %>%
    group_by(income_bracket) %>%
    summarise_all(mean)
rfc_dat_avg
```

```{r}
# export to csv to perform row analysis in python
# since I can't figure it out in R
write.csv(rfc_dat_avg, "rfc_dat_avg.csv")
```

## Logistic Regression
```{r Log Reg setup}
# PRFTLF
# Reading in the 4th file
load(file = "34453-0004-Data.rda")
employ_dat_orig <- da34453.0004

employ_dat <- employ_dat_orig %>%
    select(TUCASEID, PRFTLF)



activity_dat <- activity_dat %>%
    select(
        TUCASEID, TUACT_N, TEWHERE, TUACTDUR,
        TUT1CODE, TUT2CODE, TUT3CODE, TRCODE
    )

summary_dat <- summary_dat %>%
    select(
        TUCASEID, TEAGE, TESEX, PEEDUCA, GTMETSTA,
        TELFS, TRDPFTPT, TRERNWA, T010101:T189999
    )

head(activity_dat)
head(summary_dat)
```